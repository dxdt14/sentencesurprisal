{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a90ebe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd, tensorflow as tf\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tensorflow.keras import layers, losses, Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "from tensorflow.keras.models import Model\n",
    "import scipy.stats as stats\n",
    "from math import ceil\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d85a467",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0720ade299f4d09913a9eb5af01f47d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/6250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#read the text file\n",
    "file = open(\"yelp/train.txt\", \"r\", encoding = \"utf8\")\n",
    "file_data = file.read()\n",
    "lines = file_data.splitlines()\n",
    "file.close()\n",
    "\n",
    "data = []\n",
    "\n",
    "#only use senteces that contain the keyword\n",
    "keyword = \"\"\n",
    "for s in lines:\n",
    "    if keyword in s:\n",
    "        data.append(s)\n",
    "\n",
    "#appending a random weird sentence to see if it pops to the top of the sentence rankings        \n",
    "#data.append(\"Colorless green ideas sleep furiously .\")\n",
    "\n",
    "#data = formatting(data)\n",
    "\n",
    "#generates embedding for the sentences\n",
    "emb = SentenceTransformer('sentence-transformers/paraphrase-MiniLM-L6-v2')\n",
    "#emb = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
    "embed = emb.encode(data, show_progress_bar = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "6b7ea288",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74876f5239004a6d9a23f5b8d4411345",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/68 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "file = open(\"yelp/train.txt\", \"r\", encoding = \"utf8\")\n",
    "file_data = file.read()\n",
    "lines = file_data.splitlines()\n",
    "file.close()\n",
    "\n",
    "data = []\n",
    "\n",
    "#only use senteces that contain the keyword\n",
    "keyword = \"pizza\"\n",
    "for s in lines:\n",
    "    if keyword in s:\n",
    "        data.append(s)\n",
    "\n",
    "#appending a random weird sentence to see if it pops to the top of the sentence rankings        \n",
    "#data.append(\"Colorless green ideas sleep furiously .\")\n",
    "\n",
    "#data = formatting(data)\n",
    "\n",
    "#generates embedding for the sentences\n",
    "emb = SentenceTransformer('sentence-transformers/paraphrase-MiniLM-L6-v2')\n",
    "#emb = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
    "embed2 = emb.encode(data, show_progress_bar = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "d8cd9b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "standardscalar = MinMaxScaler()\n",
    "embed = standardscalar.fit_transform(embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "d3b2be5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 75 \n",
    "\n",
    "class Autoencoders(Model):\n",
    "    def __init__(self, latent_dim):\n",
    "        super().__init__()\n",
    "        self.encoder = Sequential(\n",
    "            [\n",
    "                Dense(175, activation = 'relu'),\n",
    "                Dense(latent_dim, activation = 'relu')\n",
    "            ]\n",
    "        )\n",
    "        self.decoder = Sequential(\n",
    "            [\n",
    "                Dense(75, activation = 'relu')\n",
    "                Dense(175, activation = 'relu'),\n",
    "                Dense(384, activation = 'sigmoid')\n",
    "            ]\n",
    "        )\n",
    "    def call(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "autoencoder = Autoencoders(latent_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "1261b2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.compile(optimizer='adam', loss='mae', metrics = ['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "c83dca30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "6250/6250 [==============================] - 9s 1ms/step - loss: 0.0589 - mae: 0.0589 - val_loss: 0.5070 - val_mae: 0.5070\n",
      "Epoch 2/15\n",
      "6250/6250 [==============================] - 8s 1ms/step - loss: 0.0525 - mae: 0.0525 - val_loss: 0.5074 - val_mae: 0.5074\n",
      "Epoch 3/15\n",
      "6250/6250 [==============================] - 8s 1ms/step - loss: 0.0509 - mae: 0.0509 - val_loss: 0.5066 - val_mae: 0.5066\n",
      "Epoch 4/15\n",
      "6250/6250 [==============================] - 8s 1ms/step - loss: 0.0499 - mae: 0.0499 - val_loss: 0.5069 - val_mae: 0.5069\n",
      "Epoch 5/15\n",
      "6250/6250 [==============================] - 8s 1ms/step - loss: 0.0495 - mae: 0.0495 - val_loss: 0.5073 - val_mae: 0.5073\n",
      "Epoch 6/15\n",
      "6250/6250 [==============================] - 8s 1ms/step - loss: 0.0491 - mae: 0.0491 - val_loss: 0.5068 - val_mae: 0.5068\n",
      "Epoch 7/15\n",
      "6250/6250 [==============================] - 8s 1ms/step - loss: 0.0489 - mae: 0.0489 - val_loss: 0.5074 - val_mae: 0.5074\n",
      "Epoch 8/15\n",
      "6250/6250 [==============================] - 8s 1ms/step - loss: 0.0487 - mae: 0.0487 - val_loss: 0.5073 - val_mae: 0.5073\n",
      "Epoch 9/15\n",
      "6250/6250 [==============================] - 8s 1ms/step - loss: 0.0486 - mae: 0.0486 - val_loss: 0.5074 - val_mae: 0.5074\n",
      "Epoch 10/15\n",
      "6250/6250 [==============================] - 8s 1ms/step - loss: 0.0485 - mae: 0.0485 - val_loss: 0.5082 - val_mae: 0.5082\n",
      "Epoch 11/15\n",
      "6250/6250 [==============================] - 8s 1ms/step - loss: 0.0484 - mae: 0.0484 - val_loss: 0.5080 - val_mae: 0.5080\n",
      "Epoch 12/15\n",
      "6250/6250 [==============================] - 8s 1ms/step - loss: 0.0483 - mae: 0.0483 - val_loss: 0.5087 - val_mae: 0.5087\n",
      "Epoch 13/15\n",
      "6250/6250 [==============================] - 8s 1ms/step - loss: 0.0482 - mae: 0.0482 - val_loss: 0.5078 - val_mae: 0.5078\n",
      "Epoch 14/15\n",
      "6250/6250 [==============================] - 8s 1ms/step - loss: 0.0481 - mae: 0.0481 - val_loss: 0.5084 - val_mae: 0.5084\n",
      "Epoch 15/15\n",
      "6250/6250 [==============================] - 8s 1ms/step - loss: 0.0481 - mae: 0.0481 - val_loss: 0.5087 - val_mae: 0.5087\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x26ea81589d0>"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoder.fit(embed, embed, epochs=15, shuffle=True, validation_data = (embed2, embed2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54038769",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "70fde48b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"autoencoders_11\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " sequential_17 (Sequential)  (None, 50)                76175     \n",
      "                                                                 \n",
      " sequential_18 (Sequential)  (None, 384)               76509     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 152,684\n",
      "Trainable params: 152,684\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "47fd07ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 0s 908us/step\n",
      "(2145, 50)\n"
     ]
    }
   ],
   "source": [
    "encoder_layer = autoencoder.get_layer(autoencoder.layers[0].name)\n",
    "reduced = encoder_layer.predict(embed2)\n",
    "print(reduced.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "a7af352e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_bins_Freedman_Diaconis(data):\n",
    "    num_bins = []\n",
    "    for i in range(data.shape[1]):\n",
    "        iqr = stats.iqr(data[:,i])\n",
    "        h = 2 * iqr * (len(data[:,i]) ** (-1/3))\n",
    "        if(np.max(data[:,i]) == 0.0):\n",
    "            print(\"Column: \", i)\n",
    "        num_bins.append(ceil((np.max(data[:,i])-np.min(data[:,i]))/h))\n",
    "    return num_bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "9a6d6fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def histogram(data, bins, frequency = False):\n",
    "    all_hist = []\n",
    "    all_bins = []\n",
    "    for i in range(data.shape[1]):\n",
    "        hist, bin_edges = np.histogram(data[:,i], bins = bins[i], density = not frequency)\n",
    "        all_hist.append(hist)\n",
    "        all_bins.append(bin_edges)\n",
    "    return all_hist, all_bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "54a234ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_bin(bins, value):\n",
    "    if value == bins[0]:\n",
    "        return 0\n",
    "    i = 1\n",
    "    while value > bins[i]:\n",
    "        i+=1\n",
    "    return i-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "acc39f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def probability(histograms, bins, data):\n",
    "    probs = []\n",
    "    for col in range(data.shape[1]):\n",
    "        col_prob = []\n",
    "        width = np.diff(np.array(bins[col]))[0]\n",
    "        for row in range(data.shape[0]):\n",
    "            idx = find_bin(bins[col], data[row][col])\n",
    "            prob = histograms[col][idx] * width\n",
    "            col_prob.append(prob)\n",
    "        probs.append(col_prob)\n",
    "    return np.array(probs).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "1a1d9506",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_surprisals(probs):\n",
    "    surprisals = []\n",
    "    for i in range(probs.shape[0]):\n",
    "        surprisals.append(-1*sum(np.log2(probs[i])))\n",
    "    return np.array(surprisals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "5bd51167",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vaj1\\AppData\\Local\\Temp\\ipykernel_19048\\3411464988.py:8: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  num_bins.append(ceil((np.max(data[:,i])-np.min(data[:,i]))/h))\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot convert float NaN to integer",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[240], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m hists, bins \u001b[38;5;241m=\u001b[39m histogram(reduced, num_bins_Freedman_Diaconis(reduced))\n\u001b[0;32m      2\u001b[0m probs \u001b[38;5;241m=\u001b[39m probability(hists, bins, reduced)\n\u001b[0;32m      3\u001b[0m sen_surprisals \u001b[38;5;241m=\u001b[39m sentence_surprisals(probs)\n",
      "Cell \u001b[1;32mIn[235], line 8\u001b[0m, in \u001b[0;36mnum_bins_Freedman_Diaconis\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m(np\u001b[38;5;241m.\u001b[39mmax(data[:,i]) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0.0\u001b[39m):\n\u001b[0;32m      7\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumn: \u001b[39m\u001b[38;5;124m\"\u001b[39m, i)\n\u001b[1;32m----> 8\u001b[0m     num_bins\u001b[38;5;241m.\u001b[39mappend(ceil((np\u001b[38;5;241m.\u001b[39mmax(data[:,i])\u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39mmin(data[:,i]))\u001b[38;5;241m/\u001b[39mh))\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m num_bins\n",
      "\u001b[1;31mValueError\u001b[0m: cannot convert float NaN to integer"
     ]
    }
   ],
   "source": [
    "hists, bins = histogram(reduced, num_bins_Freedman_Diaconis(reduced))\n",
    "probs = probability(hists, bins, reduced)\n",
    "sen_surprisals = sentence_surprisals(probs)\n",
    "rank_surprisal = np.flip(np.argsort(sen_surprisals))\n",
    "sen_rankings = []\n",
    "for i in rank_surprisal:\n",
    "    sen_rankings.append(data[i])\n",
    "with open(\"Autoencoder Sentence Rankings.txt\", 'w') as f:\n",
    "    for i in sen_rankings:\n",
    "        f.write(i + \"\\n\")\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "bb2e02aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([], dtype=int64), array([], dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "print(np.where(reduced == np.nan))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "fd181070",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(ceil(0.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "52479732",
   "metadata": {},
   "outputs": [],
   "source": [
    "zeros = []\n",
    "for i in range(reduced.shape[1]):\n",
    "    if np.max(reduced[:,i]) == 0.0:\n",
    "        zeros.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "a07be8de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 2, 6, 10, 16, 19, 24, 25, 26, 31, 33, 34, 36, 37, 39, 42, 44, 46, 47, 48]\n"
     ]
    }
   ],
   "source": [
    "print(zeros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "cf509a65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.78215295 ... 0.         0.         0.81141233]\n",
      " [0.         0.         0.73797715 ... 0.         0.         0.        ]\n",
      " [0.         0.         0.6183241  ... 0.         0.         1.8491237 ]\n",
      " ...\n",
      " [0.         0.         1.0148429  ... 0.         0.         1.8013648 ]\n",
      " [0.         0.         0.8144597  ... 0.         0.         1.1173072 ]\n",
      " [0.         0.         0.7686293  ... 0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "a6ca15bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_no_zeros = []\n",
    "for i in range(reduced.shape[1]):\n",
    "    if np.max(reduced[:,i]) != 0:\n",
    "        reduced_no_zeros.append(reduced[:,i])\n",
    "reduced_no_zeros = np.array(reduced_no_zeros).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "ab777981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2145, 32)\n"
     ]
    }
   ],
   "source": [
    "print(reduced_no_zeros.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e354d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
